---
# dev-ralph Configuration

iteration_limit: 500      # Maximum iterations before auto-stop
retry_limit: 5            # Retries before asking developer for help
coverage_threshold: 80    # Test coverage percentage required
verbosity: normal         # minimal | normal | verbose

# Anti-cheat patterns (verification will grep for these)
placeholder_patterns:
  - "TODO"
  - "FIXME"
  - "unimplemented"
  - "NotImplementedError"
  - "throw new Error('Not implemented')"

# Build commands for this project
# Set these to match your project's toolchain
build_commands:
  type_check: "bun run type-check"   # Type checking command
  lint: "bun run lint"               # Linting command
  test: "bun run test"               # Test runner command
  coverage: "bun run test:coverage"  # Test coverage command

# Integration patterns to verify (supports regex: prefix)
integration_patterns:
  # - "app.register"
  # - "regex:@(app|router)\\.(get|post|put|delete)"

# Entry points for integration verification
entry_points:
  # - "src/main.ts"

# Integration strictness: strict | warn | lenient
integration_strictness: warn
---

# PROMPT.md - Per-Item Implementation Loop

This file is fed to Claude at the start of each iteration.

## Core Principle: ONE ITEM AT A TIME

Each iteration, you implement **exactly ONE item** from the implementation plan:
1. Pick the first unchecked `[ ]` item
2. Implement it fully
3. Verify it works
4. Mark it `[x]`
5. Output: `<item>COMPLETE</item>`

The stop hook will re-feed this prompt for the next item until all items are done.

---

## Context Loading

Before starting work, read these in order:

### 1. Read Implementation Plan
Study `IMPLEMENTATION_PLAN.md` for:
- The FIRST unchecked `[ ]` item - **this is your task**
- Phase organization (which phase are you in?)
- Progress so far

### 2. Read Specs
Study `specs/*.md` for:
- Technical Contract (data models, API shapes)
- Requirements and edge cases
- Success criteria

### 3. Read stdlib Patterns (if exists)
If `stdlib/*.md` files exist, study them for:
- Project-specific code patterns
- The "right way" to do things in this codebase
- Error handling conventions

### 4. Read Lessons Learned
Study `lessons-learned.md` for:
- Patterns that worked in previous iterations
- **Error Patterns with counts** - Note `**[N]**` prefixes
- Discovered requirements

### 5. Check Signs Section Below
**CRITICAL**: Signs are hard-learned anti-patterns. ALWAYS follow them.

---

## Signs (Learned Anti-Patterns)

Signs are errors that repeated 3+ times. They are auto-promoted from lessons-learned.md.
**YOU MUST FOLLOW THESE** - they represent hard-learned lessons.

<!--
Main Claude: When you see **[3]** or higher in lessons-learned.md Error Patterns,
ADD A NEW SIGN HERE using this format:

### SIGN: {short title}
- **Problem**: {what goes wrong}
- **Solution**: {how to fix it}
- **Added**: iteration {N}

Then remove or reset the count in lessons-learned.md.
-->

---

## Your Task: ONE ITEM

**DO NOT implement multiple items.** Just the first unchecked one.

### Step 1: Identify Your Task
```bash
# Find the first unchecked item
grep -m1 '^\- \[ \]' IMPLEMENTATION_PLAN.md
```

### Step 2: Implement It Fully
- Write complete code (no placeholders!)
- Follow patterns from lessons-learned.md
- Match spec requirements exactly

### Step 3: Verify It Works
```bash
bun run type-check  # Must pass
```

### Step 4: Mark Complete
Edit IMPLEMENTATION_PLAN.md: change `[ ]` to `[x]` for this item.

### Step 5: Signal Done
Output this exact tag:
```
<item>COMPLETE</item>
```

---

## After Each Item (Compound Learning)

When you output `<item>COMPLETE</item>`, the stop hook will:
1. Trigger verification agents
2. Feed back results
3. Re-prompt you for the next item

**Your Learning Responsibilities** (before next item):

1. **Update lessons-learned.md**:
   - If new error: Add `**[1]** Error description → Fix`
   - If repeat error: Increment count `**[2]**` → `**[3]**`
   - Add successful patterns to "What Worked"

2. **Check for Sign Promotion**:
   - If any error has count `**[3]**` or higher
   - Add Sign to PROMPT.md Signs section
   - Reset/remove the count in lessons-learned.md

3. **Read updated context** before starting next item

**This is compound learning**: each iteration benefits from previous learnings.

---

## When All Items Are Done

After the LAST item is complete:
1. Run full verification: `Task(subagent_type="dev-ralph:verification-auditor", ...)`
2. Read `.ralph/verification-report.md`
3. Update `lessons-learned.md` with final learnings
4. If ALL checks pass: Output `<promise>VERIFIED_COMPLETE</promise>`
5. If ANY check fails: Fix and re-verify

---

## Anti-Cheating Rules

**NON-NEGOTIABLE:**

1. NO placeholder code (TODO, FIXME, stubs)
2. NO empty function bodies
3. NO unimplemented interfaces
4. FULL implementations only

If you cannot implement something:
- Document what's blocking you
- Ask the developer for help
- Do NOT fake it

---

## Error Recovery

If stuck:
1. Read lessons-learned.md for similar issues
2. Check if a Sign applies
3. Document what you tried
4. Ask developer for guidance

---

## Summary

```
┌─────────────────────────────────────────┐
│  ITERATION LOOP                         │
│                                         │
│  1. Read context (plan, specs, stdlib,  │
│     lessons-learned, Signs)             │
│  2. Pick FIRST unchecked item           │
│  3. Implement fully                     │
│  4. Type-check                          │
│  5. Mark [x]                            │
│  6. Output: <item>COMPLETE</item>       │
│                                         │
│  → Verification runs                    │
│  → Update lessons-learned.md            │
│  → Promote errors to Signs if count>=3  │
│  → Next iteration starts                │
│                                         │
│  UNTIL: All items done                  │
│  THEN: Final verification               │
│  THEN: <promise>VERIFIED_COMPLETE</promise>
└─────────────────────────────────────────┘
```
